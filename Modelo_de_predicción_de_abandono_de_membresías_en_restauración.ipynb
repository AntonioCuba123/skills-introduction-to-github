{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmhoptdPfyWP+u4+U0ON6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonioCuba123/skills-introduction-to-github/blob/main/Modelo_de_predicci%C3%B3n_de_abandono_de_membres%C3%ADas_en_restauraci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Simulamos un conjunto de datos para el ejemplo\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generamos 1000 clientes de ejemplo\n",
        "n_samples = 1000\n",
        "\n",
        "# Creamos características con distribuciones realistas\n",
        "data = {\n",
        "    # Demográficos\n",
        "    'edad': np.random.normal(38, 12, n_samples).astype(int),\n",
        "    'distancia_km': np.random.exponential(5, n_samples),\n",
        "    'nivel_ingresos': np.random.choice(['bajo', 'medio', 'alto', 'premium'], n_samples,\n",
        "                                      p=[0.2, 0.45, 0.25, 0.1]),\n",
        "\n",
        "    # Comportamiento\n",
        "    'visitas_mensuales': np.random.poisson(2.5, n_samples),\n",
        "    'consumo_promedio': np.random.normal(35, 15, n_samples),\n",
        "    'dias_ultima_visita': np.random.exponential(30, n_samples).astype(int),\n",
        "    'variedad_platos': np.random.poisson(3, n_samples),\n",
        "\n",
        "    # Programa\n",
        "    'meses_antiguedad': np.random.gamma(shape=5, scale=3, size=n_samples).astype(int),\n",
        "    'uso_beneficios': np.random.beta(2, 5, n_samples),  # Porcentaje de beneficios usados\n",
        "    'eventos_asistidos': np.random.poisson(1, n_samples),\n",
        "    'promociones_redimidas': np.random.poisson(3, n_samples),\n",
        "\n",
        "    # Satisfacción\n",
        "    'calificacion': np.random.normal(4, 1, n_samples).clip(1, 5),\n",
        "    'quejas_presentadas': np.random.poisson(0.3, n_samples),\n",
        "    'problemas_resueltos': np.random.binomial(1, 0.8, n_samples)  # 1 si el problema fue resuelto\n",
        "}\n",
        "\n",
        "# Convertimos a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Generamos una variable objetivo sintética basada en patrones realistas\n",
        "# Los clientes con mayor probabilidad de abandono suelen tener:\n",
        "# - Baja frecuencia de visitas\n",
        "# - Más días desde la última visita\n",
        "# - Menos uso de beneficios\n",
        "# - Más quejas y menor calificación\n",
        "\n",
        "# Calculamos un puntaje de riesgo\n",
        "risk_score = (\n",
        "    -0.3 * df['visitas_mensuales'] +\n",
        "    0.02 * df['dias_ultima_visita'] +\n",
        "    -0.5 * df['uso_beneficios'] +\n",
        "    0.7 * df['quejas_presentadas'] +\n",
        "    -0.4 * df['calificacion'] +\n",
        "    -0.1 * df['meses_antiguedad'] +\n",
        "    0.2 * (df['problemas_resueltos'] == 0)\n",
        ")\n",
        "\n",
        "# Normalizamos y convertimos a probabilidad con sigmoide\n",
        "probability = 1 / (1 + np.exp(-risk_score))\n",
        "\n",
        "# Asignamos la clase (abandono=1) con cierta aleatoriedad\n",
        "df['abandono'] = (probability > np.random.uniform(0.3, 0.7, n_samples)).astype(int)\n",
        "\n",
        "# Convertimos variables categóricas a numéricas\n",
        "df = pd.get_dummies(df, columns=['nivel_ingresos'], drop_first=True)\n",
        "\n",
        "# Dividimos en conjunto de entrenamiento y prueba\n",
        "X = df.drop('abandono', axis=1)\n",
        "y = df['abandono']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Estandarizamos las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Entrenamos un modelo de Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluamos el modelo\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculamos métricas de rendimiento\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nInforme de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")\n",
        "\n",
        "# Visualizamos la importancia de las características\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance.head(10))\n",
        "plt.title('Las 10 características más importantes')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Implementamos un sistema de puntuación para identificar clientes en riesgo\n",
        "\n",
        "\n",
        "def calcular_riesgo_cliente(datos_cliente):\n",
        "    # Creamos un DataFrame con los datos del nuevo cliente\n",
        "    # Usamos las columnas de X_train para asegurar la compatibilidad\n",
        "    nuevo_cliente_df = pd.DataFrame([datos_cliente], columns=[\n",
        "        'edad', 'distancia_km', 'visitas_mensuales', 'consumo_promedio',\n",
        "        'dias_ultima_visita', 'variedad_platos', 'meses_antiguedad',\n",
        "        'uso_beneficios', 'eventos_asistidos', 'promociones_redimidas',\n",
        "        'calificacion', 'quejas_presentadas', 'problemas_resueltos',\n",
        "        'nivel_ingresos_medio', 'nivel_ingresos_premium', 'nivel_ingresos_alto'\n",
        "    ])\n",
        "    # Get missing columns in nuevo_cliente_df compared to X_train\n",
        "    missing_cols = set(X_train.columns) - set(nuevo_cliente_df.columns)\n",
        "\n",
        "    # Add missing columns with 0 values to nuevo_cliente_df\n",
        "    for col in missing_cols:\n",
        "        nuevo_cliente_df[col] = 0\n",
        "\n",
        "    # Reordenamos las columnas para que coincidan con X_train\n",
        "    # Esto es importante para que el StandardScaler funcione correctamente\n",
        "    # Solo seleccionamos las columnas que están presentes en X_train\n",
        "    nuevo_cliente_df = nuevo_cliente_df[X_train.columns]\n",
        "\n",
        "    # Ahora podemos escalar los datos del nuevo cliente\n",
        "    datos_procesados = scaler.transform(nuevo_cliente_df)\n",
        "\n",
        "    # Predecimos la probabilidad de abandono\n",
        "    probabilidad_abandono = model.predict_proba(datos_procesados)[0, 1]\n",
        "\n",
        "    # Asignamos el nivel de riesgo\n",
        "    if probabilidad_abandono > 0.7:\n",
        "        nivel_riesgo = \"Alto\"\n",
        "        acciones = \"Contacto prioritario, oferta personalizada de valor\"\n",
        "    elif probabilidad_abandono > 0.4:\n",
        "        nivel_riesgo = \"Medio\"\n",
        "        acciones = \"Envío de promoción especial, recordatorio de beneficios\"\n",
        "    else:\n",
        "        nivel_riesgo = \"Bajo\"\n",
        "        acciones = \"Mantener comunicación regular\"\n",
        "\n",
        "    return {\n",
        "        \"probabilidad\": probabilidad_abandono,\n",
        "        \"nivel_riesgo\": nivel_riesgo,\n",
        "        \"acciones_recomendadas\": acciones\n",
        "    }\n",
        "\n",
        "# Ejemplo de uso con un cliente nuevo\n",
        "# Asegúrate de incluir los valores para las variables one-hot encoded\n",
        "# en el mismo orden en que se crearon durante el entrenamiento\n",
        "nuevo_cliente = [35, 3.5, 2, 30, 7, 3, 10, 0.5, 1, 2, 4, 0, 1, 0, 0, 1] # Ejemplo: nivel_ingresos = alto\n",
        "resultado = calcular_riesgo_cliente(nuevo_cliente)\n",
        "print(\"\\nEvaluación de riesgo del nuevo cliente:\")\n",
        "print(f\"Probabilidad de abandono: {resultado['probabilidad']:.2f}\")\n",
        "print(f\"Nivel de riesgo: {resultado['nivel_riesgo']}\")\n",
        "print(f\"Acciones recomendadas: {resultado['acciones_recomendadas']}\")"
      ],
      "metadata": {
        "id": "mKbgCr6BHyBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OqrYqtHTNHOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}